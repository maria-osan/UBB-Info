{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4deb7c10-8f11-4bd5-8719-c8fa21aaa0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "import csv \n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import DBSCAN,AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9dfae46e-9ee0-41df-a45a-152e2d01637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(subscription_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b77aae2-fc36-4fce-9185-09f3d8e6dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "textToAnalize = [\"By choosing a bike over a car, I’m reducing my environmental footprint. Cycling promotes eco-friendly transportation, and I’m proud to be part of that movement..\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b761743-4f57-40de-b8a0-e2dcb78ccadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment of this text: \n",
      "Document text: By choosing a bike over a car, I’m reducing my environmental footprint. Cycling promotes eco-friendly transportation, and I’m proud to be part of that movement..\n",
      "Overall sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "# sentiment analysis - azure client \n",
    "result = client.analyze_sentiment(textToAnalize, show_opinion_mining=True)\n",
    "docs = [doc for doc in result if not doc.is_error]\n",
    "\n",
    "print(\"Sentiment of this text: \")\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"Document text: {textToAnalize[idx]}\")\n",
    "    print(f\"Overall sentiment: {doc.sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d521e244-5904-440f-b0ee-fc2f5844dbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The rooms are extremely small, practically only a bed.', 'Room safe did not work.', 'Mattress very comfortable.']\n"
     ]
    }
   ],
   "source": [
    "def loadData(fileName):\n",
    "    data = []\n",
    "    with open(fileName) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                dataNames = row\n",
    "            else:\n",
    "                data.append(row)\n",
    "            line_count += 1\n",
    "    \n",
    "    inputs = [data[i][0] for i in range(len(data))][:100]\n",
    "    outputs = [data[i][1] for i in range(len(data))][:100]\n",
    "    labelNames = list(set(outputs))\n",
    "    \n",
    "    return inputs, outputs, labelNames\n",
    "\n",
    "crtDir = os.getcwd()\n",
    "fileName = os.path.join(crtDir, 'data/reviews_mixed.csv')\n",
    "\n",
    "inputs, outputs, labelNames = loadData(fileName)\n",
    "print(inputs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09e1b318-111f-44f1-8d46-629bd7921895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split  the data in train and test\n",
    "def splitData(inputs, outputs):\n",
    "    np.random.seed(5)\n",
    "    noSamples = len(inputs)\n",
    "    indexes = [i for i in range(noSamples)]\n",
    "    trainSample = np.random.choice(indexes, int(0.8 * noSamples), replace=False)\n",
    "    testSample = [i for i in indexes if not i in trainSample]\n",
    "    \n",
    "    trainInputs = [inputs[i] for i in trainSample]\n",
    "    trainOutputs = [outputs[i] for i in trainSample]\n",
    "    testInputs = [inputs[i] for i in testSample]\n",
    "    testOutputs = [outputs[i] for i in testSample]\n",
    "\n",
    "    return trainInputs, trainOutputs, testInputs, testOutputs\n",
    "\n",
    "trainInputs, trainOutputs, testInputs, testOutputs = splitData(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "776a4559-5b27-452a-ae9e-709f5acd2437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 341 words\n",
      "Train data size: 80 emails\n",
      "Train features shape: (80, 341)\n",
      "Some words of the vocab: ['ran', 'until', 'jiggled', 'handle', 'showers', 'renovations', 'they', 'seem', 'anything', 'else', 'bedroom', 'sofa', 'unconfortable', 'springy', 'everything', 'beds', 'greatest', 'futon', 'sleeper', 'couch']\n",
      "Some features: [[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# extract some features from the raw text\n",
    "# Bag of Words\n",
    "def extractFeaturesBoW(trainInputs, testInputs):\n",
    "    vectorizer = CountVectorizer()\n",
    "    trainFeatures = vectorizer.fit_transform(trainInputs)\n",
    "    testFeatures = vectorizer.transform(testInputs)\n",
    "    \n",
    "    # Additional information\n",
    "    # vocabulary from the train data \n",
    "    vocabWords = list(vectorizer.vocabulary_.keys())[-20:]\n",
    "    # extracted features\n",
    "    sampleFeatures = trainFeatures.toarray()[:3]\n",
    "    \n",
    "    print(\"Vocabulary size:\", len(vectorizer.vocabulary_), \"words\")\n",
    "    print(\"Train data size:\", len(trainInputs), \"emails\")\n",
    "    # shape of feature matrix\n",
    "    print(\"Train features shape:\", trainFeatures.shape)\n",
    "    print(\"Some words of the vocab:\", vocabWords)\n",
    "    print(\"Some features:\", sampleFeatures)\n",
    "    \n",
    "    return trainFeatures, testFeatures, vectorizer\n",
    "\n",
    "trainFeatures, testFeatures, vectorizer = extractFeaturesBoW(trainInputs, testInputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd80b662-3a94-492c-96d1-cad3361720cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['all', 'and', 'are', 'area', 'bathroom', 'bed', 'bit', 'clean', 'cold', 'comfortable']\n",
      "Features: [[0.         0.14603507 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.51211449 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.34685183 0.19759403 0.20555329 0.20555329 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.3238264  0.         0.17861231 0.         0.         0.\n",
      "  0.45121804 0.         0.         0.         0.19759403 0.\n",
      "  0.22560902 0.20555329]\n",
      " [0.         0.81777684 0.         0.         0.         0.\n",
      "  0.         0.57553543 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# extract some features from the raw text\n",
    "# tf-idf features - word granularity\n",
    "def extractFeatures_tf_idf(trainInputs, testInputs):\n",
    "    vectorizer = TfidfVectorizer(max_features=50)\n",
    "    trainFeatures = vectorizer.fit_transform(trainInputs)\n",
    "    testFeatures = vectorizer.transform(testInputs)\n",
    "    \n",
    "    # Additional information\n",
    "    # vocabulary from the train data \n",
    "    vocabWords = list(vectorizer.get_feature_names_out())[:10]\n",
    "    # extracted features\n",
    "    sampleFeatures = trainFeatures.toarray()[:3]\n",
    "    \n",
    "    print(\"Vocabulary:\", vocabWords)\n",
    "    print(\"Features:\", sampleFeatures)\n",
    "    \n",
    "    return trainFeatures, testFeatures, vectorizer\n",
    "\n",
    "trainFeatures, testFeatures, vectorizer = extractFeatures_tf_idf(trainInputs, testInputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "331f8ab6-04a0-469d-9dc8-0c656718e467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['aircon' 'and' 'are' 'bathroom' 'bed' 'building' 'cleaned' 'comfortable'\n",
      " 'filthy' 'for']\n",
      "Features:  [[0.         0.14603507 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.51211449 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.34685183 0.19759403 0.20555329 0.20555329 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.3238264  0.         0.17861231 0.         0.         0.\n",
      "  0.45121804 0.         0.         0.         0.19759403 0.\n",
      "  0.22560902 0.20555329]\n",
      " [0.         0.81777684 0.         0.         0.         0.\n",
      "  0.         0.57553543 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def featureComputation(vectorizer, data):\n",
    "    # Fit vectorizer on data to learn vocabulary and compute TF-IDF features\n",
    "    features = vectorizer.fit_transform(data).toarray()\n",
    "    return features\n",
    "    \n",
    "trainFeatures = featureComputation(vectorizer, trainInputs)\n",
    "testFeatures = featureComputation(vectorizer, testInputs)\n",
    "\n",
    "print('Vocabulary: ', vectorizer.get_feature_names_out()[:10])\n",
    "print('Features: ', trainFeatures[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e94df142-4697-45f5-b104-a71f68eaed9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bed is very comfortable.  ->  positive\n",
      "Very spacious rooms, quiet and very comfortable.  ->  positive\n",
      "Corridors filthy\n",
      "Room filthy\n",
      "Electrical cables in room not safe\n",
      "Whole building smelly\n",
      "Shower repulsive  ->  positive\n",
      "walls seem to have no sound insulation  ->  positive\n",
      "The building was under renovation,  ->  positive\n",
      "no elevator might be a challenge for some people  ->  negative\n",
      "The bed was highly uncomfortable, although the engineer fixed it  ->  positive\n",
      "bed, smell.  ->  positive\n",
      "Detest the glass \"door\" if shower/tub .. with?  ->  positive\n",
      "this was expected, clean towels and room cleaned every day.  ->  positive\n",
      "More plug outlets with surge protectors.  ->  positive\n",
      "Room was very spacious  ->  negative\n",
      "Roof terrace great  ->  positive\n",
      "No tea or coffee making facilities in the rooms  ->  positive\n",
      "the room had aircon and we had earplugs and slept soundly.  ->  positive\n",
      "Also, when the bright bathroom lights are turned on, it lights up the whole hotel room, shining thru the frosted glass panels.  ->  positive\n",
      "Bathroom was extra small,  ->  positive\n",
      "Wifi connected  ->  positive\n",
      "Windows haven't been cleaned for years (if ever).  ->  positive\n",
      "No wardrobe, no space for luggage, no towel change, walls are not sound proof thus very noisy.  ->  positive\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "kmeans.fit(trainFeatures)\n",
    "\n",
    "computedTestIndexes = kmeans.predict(testFeatures)\n",
    "computedTestOutputs = [labelNames[value] for value in computedTestIndexes]\n",
    "for i in range(0, len(testInputs)):\n",
    "    print(testInputs[i], \" -> \", computedTestOutputs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fd12193-05f9-4198-9c96-0f7525a40ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before the new feature:  0.3\n"
     ]
    }
   ],
   "source": [
    "# calculate performance\n",
    "print(\"Accuracy before the new feature: \", accuracy_score(testOutputs, computedTestOutputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc8fafe9-c81b-4453-9aeb-83b5bcb34cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  positive\n"
     ]
    }
   ],
   "source": [
    "# make the prediction for our text \n",
    "textFeatures = vectorizer.transform(textToAnalize)\n",
    "textClusterIndex = kmeans.predict(textFeatures)[0]\n",
    "computedTestOutputs = labelNames[textClusterIndex]\n",
    "print(\"Prediction: \", computedTestOutputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c120a25-89d2-4523-af32-358b4644bed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN Accuracy: 0.3\n",
      "Agglomerative Clustering Accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "# Alternativs for k-Means:\n",
    "def alternativeClustering(trainFeatures, testFeatures, testOutputs, labelNames):\n",
    "    # Dbscan\n",
    "    # min_sample nr of points to be in a region for beeing a cluster\n",
    "    # 0.5 the radius between 2 points to be considerated togheter\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    dbscanLabelsTrain = dbscan.fit_predict(trainFeatures)\n",
    "\n",
    "    # Performance\n",
    "    dbscanLabelsTest = dbscan.fit_predict(testFeatures)\n",
    "    dbscanTestOutputs = [labelNames[value] for value in dbscanLabelsTest]\n",
    "    dbscanAccuracy = accuracy_score(testOutputs, dbscanTestOutputs)\n",
    "    print(\"DBSCAN Accuracy:\", dbscanAccuracy)\n",
    "    \n",
    "    # Agglomerative Clustering\n",
    "    # n - clusters = 2 (positive / negative)\n",
    "    agglo = AgglomerativeClustering(n_clusters=2)\n",
    "    aggloLabelsTrain = agglo.fit_predict(trainFeatures)\n",
    "    \n",
    "    aggloLabelsTest = agglo.fit_predict(testFeatures)\n",
    "    aggloTestOutputs = [labelNames[value] for value in aggloLabelsTest]\n",
    "    aggloAccuracy = accuracy_score(testOutputs, aggloTestOutputs)\n",
    "    print(\"Agglomerative Clustering Accuracy:\", aggloAccuracy)\n",
    "    \n",
    "    return dbscanAccuracy, aggloAccuracy\n",
    "\n",
    "dbscanAccuracy, aggloAccuracy = alternativeClustering(trainFeatures, testFeatures, testOutputs, labelNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e378cc-a510-4ccc-a51e-349ac08e0982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
